{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph 핸즈온\n",
    "\n",
    "**참고하면 좋은 자료**\n",
    "\n",
    "- [LangChain 한국어 튜토리얼🇰🇷](https://wikidocs.net/book/14314)\n",
    "- [LangChain 한국어 튜토리얼 Github 소스코드](https://github.com/teddylee777/langchain-kr)\n",
    "- [테디노트 YouTube](https://www.youtube.com/c/@teddynote)\n",
    "- [테디노트 블로그](https://teddylee777.github.io/)\n",
    "- [테디노트 YouTube 로 RAG 배우기!](https://teddylee777.notion.site/YouTube-RAG-10a24f35d12980dc8478c750faa752a2?pvs=74)\n",
    "- [RAG 비법노트](https://fastcampus.co.kr/data_online_teddy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. 환경 설정\n",
    "\n",
    "**OpenAI API Key 설정**\n",
    "- https://wikidocs.net/233342\n",
    "\n",
    "**웹 검색을 위한 API 키 발급 주소**\n",
    "- https://app.tavily.com/\n",
    "\n",
    "회원 가입 후 API Key를 발급합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**설치를 진행합니다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langgraph langchain_openai langchain_teddynote faiss-cpu pdfplumber langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실습자료를 다운로드 받습니다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # 발급 받은 OpenAI API Key 입력\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"\"  # 발급 받은 Tavily API Key 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(선택 사항)\n",
    "\n",
    "LangSmith 추적을 원하는 경우 아래 LangSmith API Key 를 발급 받아 입력해 주세요.\n",
    "\n",
    "- 링크: https://smith.langchain.com\n",
    "- 회원 가입 후 - 설정 - 상단 API Keys 에서 발급"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"  # 발급 받은 LangSmith API Key 입력\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"  # 추적 설정\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"  # 추적 엔드포인트\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"LangGraph-Hands-On\"  # 프로젝트 이름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. 기본 ReAct Agent 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 모델 설정\n",
    "model = ChatOpenAI(model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 도구 (Tools) 설정\n",
    "\n",
    "도구(Tool)는 에이전트, 체인 또는 LLM이 외부 세계와 상호작용하기 위한 인터페이스입니다.\n",
    "\n",
    "LangChain 에서 기본 제공하는 도구를 사용하여 쉽게 도구를 활용할 수 있으며, 사용자 정의 도구(Custom Tool) 를 쉽게 구축하는 것도 가능합니다.\n",
    "\n",
    "**LangChain 에 통합된 도구 리스트는 아래 링크에서 확인할 수 있습니다.**\n",
    "\n",
    "랭체인에서 제공하는 사전에 정의된 도구(tool) 와 툴킷(toolkit) 을 사용할 수 있습니다.\n",
    "\n",
    "tool 은 단일 도구를 의미하며, toolkit 은 여러 도구를 묶어서 하나의 도구로 사용할 수 있습니다.\n",
    "\n",
    "관련 도구는 아래의 링크에서 참고하실 수 있습니다.\n",
    "\n",
    "**참고**\n",
    "- [LangChain Tools/Toolkits](https://python.langchain.com/docs/integrations/tools/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**검색 API 도구**\n",
    "\n",
    "Tavily 검색 API를 활용하여 검색 기능을 구현하는 도구입니다. \n",
    "\n",
    "**API 키 발급 주소**\n",
    "- https://app.tavily.com/\n",
    "\n",
    "발급한 API 키를 환경변수에 설정합니다.\n",
    "\n",
    "`.env` 파일에 아래와 같이 설정합니다.\n",
    "\n",
    "```\n",
    "TAVILY_API_KEY=tvly-abcdefghijklmnopqrstuvwxyz\n",
    "```\n",
    "\n",
    "**TavilySearch**\n",
    "\n",
    "**설명**\n",
    "- Tavily 검색 API를 쿼리하고 JSON 형식의 결과를 반환합니다.\n",
    "- 포괄적이고 정확하며 신뢰할 수 있는 결과에 최적화된 검색 엔진입니다.\n",
    "- 현재 이벤트에 대한 질문에 답변할 때 유용합니다.\n",
    "\n",
    "**주요 매개변수**\n",
    "- `max_results` (int): 반환할 최대 검색 결과 수 (기본값: 5)\n",
    "- `search_depth` (str): 검색 깊이 (\"basic\" 또는 \"advanced\")\n",
    "- `include_domains` (List[str]): 검색 결과에 포함할 도메인 목록\n",
    "- `exclude_domains` (List[str]): 검색 결과에서 제외할 도메인 목록\n",
    "- `include_answer` (bool): 원본 쿼리에 대한 짧은 답변 포함 여부\n",
    "- `include_raw_content` (bool): 각 사이트의 정제된 HTML 콘텐츠 포함 여부\n",
    "- `include_images` (bool): 쿼리 관련 이미지 목록 포함 여부\n",
    "\n",
    "**반환 값**\n",
    "- 검색 결과를 포함하는 JSON 형식의 문자열(url, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools.tavily import TavilySearch\n",
    "\n",
    "# 웹 검색 도구를 설정합니다.\n",
    "web_search_tool = TavilySearch(\n",
    "    max_results=6,  # 최대 검색 결과\n",
    ")\n",
    "\n",
    "# 웹 검색 도구의 이름과 설명을 설정합니다.\n",
    "web_search_tool.name = \"web_search\"\n",
    "web_search_tool.description = \"Use this tool to search on the web\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PDFRetrievalChain`: PDF 문서 기반 Naive RAG 체인\n",
    "\n",
    "문서 기반 RAG 체인을 생성합니다. 이 체인은 주어진 PDF 문서를 기반으로 검색 기능을 제공합니다.\n",
    "\n",
    "**주요 매개변수**\n",
    "- `source_uri` (List[str]): PDF 문서의 경로\n",
    "- `model_name` (str): 사용할 모델의 이름\n",
    "- `k` (int): 반환할 최대 검색 결과 수 (기본값: 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.pdf import PDFRetrievalChain\n",
    "\n",
    "# PDF 문서를 로드합니다.\n",
    "pdf = PDFRetrievalChain(\n",
    "    [\"data/SPRI_AI_Brief_2023년12월호_F.pdf\"], model_name=\"gpt-4o-mini\", k=6\n",
    ").create_chain()\n",
    "\n",
    "# retriever와 chain을 생성합니다.\n",
    "pdf_retriever = pdf.retriever\n",
    "pdf_chain = pdf.chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 쿼리를 입력하여 검색 결과를 반환합니다.\n",
    "searched_docs = pdf_retriever.invoke(\"삼성전자가 만든 생성형 AI 이름을 찾아줘\")\n",
    "searched_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**추적**: https://smith.langchain.com/public/bdaa2410-0a6a-44c9-8e2b-c5d8628bf84e/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = pdf_chain.invoke(\n",
    "    {\"question\": \"삼성전자가 만든 생성형 AI 이름을 찾아줘\", \"context\": searched_docs}\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "\n",
    "# PDF 문서를 기반으로 검색 도구 생성\n",
    "retriever_tool = create_retriever_tool(\n",
    "    pdf_retriever,\n",
    "    \"pdf_retriever\",\n",
    "    \"Search and return information about SPRI AI Brief PDF file. It contains useful information on recent AI trends. The document is published on Dec 2023.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retriever_tool.invoke(\"삼성전자가 만든 생성형 AI 이름을 찾아줘\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "도구 목록을 정의 합니다. 이는 Agent 에게 제공될 도구 목록입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도구 목록 정의\n",
    "tools = [web_search_tool, retriever_tool]\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_react_agent`\n",
    "\n",
    "ReAct Agent 를 생성합니다. 이는 도구 목록을 제공하고, 사용자의 질문에 대한 답변을 생성합니다.\n",
    "\n",
    "- `model`: 사용할 모델\n",
    "- `tools`: 도구 목록\n",
    "- `prompt`: 시스템 프롬프트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "simple_react_agent = create_react_agent(\n",
    "    model, tools, prompt=\"You are a helpful assistant. Answer in Korean.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그래프 시각화**\n",
    "\n",
    "`visualize_graph` 함수는 그래프를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(simple_react_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 실행\n",
    "\n",
    "- `config` 파라미터는 그래프 실행 시 필요한 설정 정보를 전달합니다.\n",
    "    - `resursion_limit`: 그래프 실행 시 재귀 최대 횟수를 설정합니다.\n",
    "    - `thread_id`: 그래프 실행 시 스레드 아이디를 설정합니다.\n",
    "- `inputs`: 그래프 실행 시 필요한 입력 정보를 전달합니다.\n",
    "\n",
    "**참고**\n",
    "\n",
    "- 메시지 출력 스트리밍은 [LangGraph 스트리밍 모드의 모든 것](https://wikidocs.net/265770) 을 참고해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "# Config 설정\n",
    "config = {\"configurable\": {\"resursion_limit\": 10, \"thread_id\": \"abc123\"}}\n",
    "\n",
    "# 입력 설정\n",
    "inputs = {\n",
    "    \"messages\": [(\"human\", \"AI Brief 문서에서 삼성전자가 만든 생성형 AI 이름을 찾아줘\")]\n",
    "}\n",
    "\n",
    "# 그래프 스트림\n",
    "stream_graph(simple_react_agent, inputs, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추적: https://smith.langchain.com/public/145d8012-9791-4320-a17d-b2ec048d0110/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**참고**: `config` 는 이전의 값을 재활용 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 스트림\n",
    "stream_graph(\n",
    "    simple_react_agent,\n",
    "    {\"messages\": [(\"human\", \"claude 3.7 sonnet 관련 정보를 검색해줘\")]},\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**추적**: https://smith.langchain.com/public/e28f6b6c-463c-4211-8a75-3c88dfdcc41c/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. 멀티턴 대화를 위한 단기 메모리: `checkpointer`\n",
    "\n",
    "단기 메모리 기능이 없는 그래프는 이전 대화를 기억하지 못합니다.\n",
    "\n",
    "즉, 멀티턴 대화를 지원하지 않는다는 말이기도 합니다. 따라서, 다음과 같이 이전 대화를 기억하지 못합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 스트림\n",
    "stream_graph(\n",
    "    simple_react_agent,\n",
    "    {\"messages\": [(\"human\", \"안녕, 반가워! 내 이름은 테디야!\")]},\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 스트림\n",
    "stream_graph(simple_react_agent, {\"messages\": [(\"human\", \"내 이름이 뭐라고?\")]}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `MemorySaver`\n",
    "\n",
    "LangGraph 는 `Checkpointer` 를 사용해 각 단계가 끝난 후 그래프 상태를 자동으로 저장할 수 있습니다.\n",
    "\n",
    "이 내장된 지속성 계층은 메모리를 제공하여 LangGraph가 마지막 상태 업데이트에서 선택할 수 있도록 합니다. \n",
    "\n",
    "가장 사용하기 쉬운 체크포인터 중 하나는 그래프 상태를 위한 인메모리 키-값 저장소인 `MemorySaver`입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 메모리 설정\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# ReAct Agent 생성(checkpointer 설정)\n",
    "simple_react_agent = create_react_agent(\n",
    "    model,\n",
    "    tools,\n",
    "    checkpointer=memory,\n",
    "    prompt=\"You are a helpful assistant. Answer in Korean.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config 설정\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "# 그래프 스트림\n",
    "stream_graph(\n",
    "    simple_react_agent,\n",
    "    {\"messages\": [(\"human\", \"안녕, 반가워! 내 이름은 테디야!\")]},\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 이전 대화 내용을 잘 기억하는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 스트림\n",
    "stream_graph(simple_react_agent, {\"messages\": [(\"human\", \"내 이름이 뭐라고?\")]}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**추적**: https://smith.langchain.com/public/16105167-e6db-4e26-add8-96cbf53191f7/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. LangGraph 워크플로우 구현\n",
    "\n",
    "이전의 `create_react_agent` 를 사용하여 에이전트를 구현해 보았습니다.\n",
    "\n",
    "하지만, 이전의 에이전트는 단일 에이전트 형태이기 때문에 복잡한 워크플로우를 구현하기 어렵습니다.\n",
    "\n",
    "이를 해결하기 위해서는 워크플로우를 구현해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "1. State 정의(TypedDict 형식으로 정의)\n",
    "2. 노드 정의(함수로 구현)\n",
    "3. 그래프 생성(StateGraph 클래스 사용)\n",
    "4. 컴파일(checkpointer 설정)\n",
    "5. 실행\n",
    "\n",
    "### State 정의\n",
    "\n",
    "`State`: Graph 의 노드와 노드 간 공유하는 상태를 정의합니다.\n",
    "\n",
    "일반적으로 `TypedDict` 형식을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# GraphState 상태 정의\n",
    "class GraphState(TypedDict):\n",
    "    question: Annotated[str, \"User's Question\"]  # 질문\n",
    "    documents: Annotated[str, \"Retrieved Documents\"]  # 문서의 검색 결과\n",
    "    answer: Annotated[str, \"LLM generated answer\"]  # 답변\n",
    "    messages: Annotated[list, add_messages]  # 메시지(누적되는 list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 노드(Node) 정의\n",
    "\n",
    "- `Nodes`: 각 단계를 처리하는 노드입니다. 보통은 Python 함수로 구현합니다. 입력과 출력이 상태(State) 값입니다.\n",
    "  \n",
    "**참고**  \n",
    "\n",
    "- `State`를 입력으로 받아 정의된 로직을 수행한 후 업데이트된 `State`를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.utils import format_docs\n",
    "\n",
    "\n",
    "# 문서 검색 노드\n",
    "def retrieve_document(state: GraphState) -> GraphState:\n",
    "    # 질문을 상태에서 가져옵니다.\n",
    "    latest_question = state[\"question\"]\n",
    "\n",
    "    # 문서에서 검색하여 관련성 있는 문서를 찾습니다.\n",
    "    retrieved_docs = pdf_retriever.invoke(latest_question)\n",
    "\n",
    "    # 검색된 문서를 형식화합니다.(프롬프트 입력으로 넣어주기 위함)\n",
    "    retrieved_docs = format_docs(retrieved_docs)\n",
    "\n",
    "    # 검색된 문서를 context 키에 저장합니다.\n",
    "    return {\"documents\": retrieved_docs}\n",
    "\n",
    "\n",
    "# 답변 생성 노드\n",
    "def llm_answer(state: GraphState) -> GraphState:\n",
    "    # 질문을 상태에서 가져옵니다.\n",
    "    latest_question = state[\"question\"]\n",
    "\n",
    "    # 검색된 문서를 상태에서 가져옵니다.\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # 체인을 호출하여 답변을 생성합니다.\n",
    "    response = pdf_chain.invoke({\"question\": latest_question, \"context\": documents})\n",
    "    # 생성된 답변, (유저의 질문, 답변) 메시지를 상태에 저장합니다.\n",
    "    return {\n",
    "        \"answer\": response,\n",
    "        \"messages\": [(\"user\", latest_question), (\"assistant\", response)],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 생성\n",
    "\n",
    "- `StateGraph`: `State` 를 입력으로 받아 `Node` 를 실행하고 `State` 를 업데이트하는 그래프 생성 클래스.\n",
    "- `Edges`: 현재 `State`를 기반으로 다음에 실행할 `Node`를 결정.\n",
    "- `set_entry_point`: 그래프 진입점 설정.\n",
    "- `compile`: 그래프 컴파일."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 그래프 생성\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드 정의\n",
    "workflow.add_node(\"retrieve\", retrieve_document)\n",
    "workflow.add_node(\"llm_answer\", llm_answer)\n",
    "\n",
    "# 엣지 정의\n",
    "workflow.add_edge(\"retrieve\", \"llm_answer\")  # 검색 -> 답변\n",
    "workflow.add_edge(\"llm_answer\", END)  # 답변 -> 종료\n",
    "\n",
    "# 그래프 진입점(entry_point) 설정\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "# 체크포인터 설정\n",
    "memory = MemorySaver()\n",
    "\n",
    "# 컴파일\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컴파일이 완료된 그래프를 실행하고 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프 실행\n",
    "\n",
    "- `config` 파라미터는 그래프 실행 시 필요한 설정 정보를 전달합니다.\n",
    "- `recursion_limit`: 그래프 실행 시 재귀 최대 횟수를 설정합니다.\n",
    "- `inputs`: 그래프 실행 시 필요한 입력 정보를 전달합니다.\n",
    "\n",
    "**참고**\n",
    "\n",
    "- 메시지 출력 스트리밍은 [LangGraph 스트리밍 모드의 모든 것](https://wikidocs.net/265770) 을 참고해주세요.\n",
    "\n",
    "아래의 `stream_graph` 함수는 특정 노드만 스트리밍 출력하는 함수입니다.\n",
    "\n",
    "손쉽게 특정 노드의 스트리밍 출력을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_graph, random_uuid\n",
    "\n",
    "# config 설정(재귀 최대 횟수, thread_id)\n",
    "config = {\"configurable\": {\"resursion_limit\": 10, \"thread_id\": random_uuid()}}\n",
    "\n",
    "# 질문 입력\n",
    "inputs = {\"question\": \"앤스로픽에 투자한 기업과 투자금액을 알려주세요.\"}\n",
    "\n",
    "# 그래프 실행\n",
    "stream_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추적: https://smith.langchain.com/public/1aa445e0-672e-4f15-9253-1c8efcdb1355/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4.Routing\n",
    "\n",
    "LLM 애플리케이션에서 라우팅은 입력 쿼리나 상태에 따라 적절한 처리 경로나 구성 요소로 요청을 전달하는 메커니즘입니다. \n",
    "\n",
    "LangChain/LangGraph에서 라우팅은 특정 작업에 가장 적합한 모델이나 도구를 선택하고, 복잡한 워크플로우를 관리하며, 비용과 성능 균형을 최적화하는 데 필수적입니다. \n",
    "\n",
    "**Agent**\n",
    "- 도구 선택을 하는 방식으로 라우팅\n",
    "- 따라서, 도우에 대한 description 이 상세하게 작성되어야 합니다.\n",
    "\n",
    "**LLM.with_structured_output**\n",
    "- Function Calling 을 사용하는 방식으로 라우팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# 사용자 쿼리를 가장 관련성 높은 데이터 소스로 라우팅하는 데이터 모델\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    # 데이터 소스 선택을 위한 리터럴 타입 필드\n",
    "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to `web_search` or a `vectorstore`.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# llm 구조화된 출력 설정\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# 프롬프트 설정\n",
    "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to AI Brief Report(SPRI) including Samsung Gause, Anthropic, etc.\n",
    "Use the vectorstore for questions on AI related topics. Otherwise, use `web_search`.\"\"\"\n",
    "\n",
    "# Routing 을 위한 프롬프트 템플릿 생성\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 프롬프트 템플릿과 구조화된 LLM 라우터를 결합하여 질문 라우터 생성\n",
    "question_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "쿼리를 실행한 뒤 호출 결과의 차이를 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리 실행\n",
    "question_router.invoke(\"삼성전자가 만든 생성형 AI 이름을 찾아줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리 실행\n",
    "question_router.invoke(\"LangCon2025 이벤트의 날짜와 장소는?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 `retrieve`, `generate`, `web_search` 노드를 구현한 코드입니다.\n",
    "\n",
    "- `retrieve`: 문서 검색 노드\n",
    "- `generate`: 답변 생성 노드\n",
    "- `web_search`: 웹 검색 노드\n",
    "\n",
    "이 노드들을 사용하여 워크플로우를 구현합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# 문서 검색 노드\n",
    "def retrieve(state):\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 문서 검색 수행\n",
    "    documents = pdf_retriever.invoke(question)\n",
    "\n",
    "    # 검색된 문서 반환\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "# 답변 생성 노드\n",
    "def generate(state):\n",
    "    # 질문과 문서 검색 결과 가져오기\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG 답변 생성\n",
    "    generation = pdf_chain.invoke({\"context\": documents, \"question\": question})\n",
    "\n",
    "    # 생성된 답변 반환\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "\n",
    "# 웹 검색 노드\n",
    "def web_search(state):\n",
    "    # print(\"==== [WEB SEARCH] ====\")\n",
    "    # 질문과 문서 검색 결과 가져오기\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # 웹 검색 수행\n",
    "    web_results = web_search_tool.invoke({\"query\": question})\n",
    "\n",
    "    # 검색된 문서 반환\n",
    "    web_results_docs = [\n",
    "        Document(\n",
    "            page_content=web_result[\"content\"],\n",
    "            metadata={\"source\": web_result[\"url\"]},\n",
    "        )\n",
    "        for web_result in web_results\n",
    "    ]\n",
    "    return {\"documents\": web_results_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "질문 라우팅 노드의 구현입니다.\n",
    "\n",
    "사용자의 질문에 대해 `question_router` 를 호출하여 적절한 데이터 소스로 라우팅합니다.\n",
    "\n",
    "- `web_search`: 웹 검색\n",
    "- `vectorstore`: 벡터 스토어\n",
    "\n",
    "라우팅 결과에 따라 적절한 노드로 라우팅합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 라우팅 노드\n",
    "def route_question(state):\n",
    "    print(\"==== [ROUTE QUESTION] ====\")\n",
    "    # 질문 가져오기\n",
    "    question = state[\"question\"]\n",
    "    # 질문 라우팅\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    # 질문 라우팅 결과에 따른 노드 라우팅\n",
    "    if source.datasource == \"web_search\":\n",
    "        print(\"\\n==== [GO TO WEB SEARCH] ====\")\n",
    "        return \"need to search web\"\n",
    "    elif source.datasource == \"vectorstore\":\n",
    "        print(\"\\n==== [GO TO VECTORSTORE] ====\")\n",
    "        return \"search on DB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그래프 생성\n",
    "\n",
    "이 단계에서는 `web_search`, `retrieve`, `generate` 노드를 생성하고, 이들을 연결하는 조건부 엣지를 추가합니다.\n",
    "\n",
    "- `web_search`: 웹 검색 노드\n",
    "- `retrieve`: 문서 검색 노드\n",
    "- `generate`: 답변 생성 노드\n",
    "\n",
    "조건부 엣지: 질문 라우팅 노드에서 반환된 결과에 따라 적절한 노드로 라우팅합니다.\n",
    "\n",
    "- `need to search web`: 웹 검색 노드로 라우팅\n",
    "- `search on DB`: 벡터 스토어 노드로 라우팅\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 그래프 상태 초기화\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드 정의\n",
    "workflow.add_node(\"web_search\", web_search)  # 웹 검색\n",
    "workflow.add_node(\"retrieve\", retrieve)  # 문서 검색\n",
    "workflow.add_node(\"generate\", generate)  # 답변 생성\n",
    "\n",
    "# 그래프 빌드\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"need to search web\": \"web_search\",  # 웹 검색으로 라우팅\n",
    "        \"search on DB\": \"retrieve\",  # 벡터스토어로 라우팅\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")  # 문서 검색 후 답변 생성\n",
    "workflow.add_edge(\"web_search\", \"generate\")  # 웹 검색 후 답변 생성\n",
    "workflow.add_edge(\"generate\", END)  # 답변 생성 후 종료\n",
    "\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행하고 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"resursion_limit\": 10, \"thread_id\": \"123\"}}\n",
    "\n",
    "stream_graph(\n",
    "    app, {\"question\": \"앤스로픽에 투자한 기업과 투자금액을 알려주세요.\"}, config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"resursion_limit\": 10, \"thread_id\": \"123\"}}\n",
    "\n",
    "stream_graph(\n",
    "    app,\n",
    "    {\"question\": \"Claude 3.7 sonnet 관련 최신 뉴스를 검색해줘. 한글로 답변해줘\"},\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5.Fan-out / Fan-in\n",
    "\n",
    "LangGraph에서 Fan-out/Fan-in은 복잡한 LLM 워크플로우 관리를 위한 중요한 패턴입니다.\n",
    "\n",
    "Fan-out은 단일 입력을 여러 병렬 작업으로 분배하는 패턴으로, 하나의 프롬프트나 쿼리를 여러 LLM, 도구, 또는 처리 단계로 동시에 전송하여 다양한 관점이나 접근 방식을 얻을 수 있게 합니다. 이는 복잡한 문제를 더 작고 전문화된 하위 작업으로 분할하거나 동일한 작업에 대해 여러 모델의 결과를 비교할 때 유용합니다.\n",
    "\n",
    "Fan-in은 Fan-out의 역과정으로, 여러 병렬 작업의 결과를 단일 출력이나 다음 단계로 통합합니다. 이는 다양한 모델이나 도구에서 생성된 결과를 종합하여 더 완전하고 정확한 최종 응답을 만들거나 여러 에이전트의 작업을 조정할 때 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# 상태 정의(add_messages 리듀서 사용)\n",
    "class State(TypedDict):\n",
    "    aggregate: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# 노드 값 반환 클래스\n",
    "class ReturnNodeValue:\n",
    "    # 초기화\n",
    "    def __init__(self, node_secret: str):\n",
    "        self._value = node_secret\n",
    "\n",
    "    # 호출시 상태 업데이트\n",
    "    def __call__(self, state: State) -> Any:\n",
    "        print(f\"Adding {self._value} to {state['aggregate']}\")\n",
    "        return {\"aggregate\": [self._value]}\n",
    "\n",
    "\n",
    "# 상태 그래프 초기화\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# 노드 A부터 D까지 생성 및 값 할당\n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "\n",
    "# 노드 연결\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"d\")\n",
    "builder.add_edge(\"c\", \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행\n",
    "result = graph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\n",
    "print(\"===\" * 30)\n",
    "print(result[\"aggregate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 일부만 Fan-out 하는 방법\n",
    "\n",
    "(Fan-out 의 순서 조정)\n",
    "\n",
    "조건부 엣지를 두어 일부만 Fan-out 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "\n",
    "\n",
    "# 상태 정의(add_messages 리듀서 사용)\n",
    "class State(TypedDict):\n",
    "    aggregate: Annotated[list, add_messages]\n",
    "    which: str\n",
    "\n",
    "\n",
    "# 노드별 고유 값을 반환하는 클래스\n",
    "class ReturnNodeValue:\n",
    "    def __init__(self, node_secret: str):\n",
    "        self._value = node_secret\n",
    "\n",
    "    def __call__(self, state: State) -> Any:\n",
    "        print(f\"Adding {self._value} to {state['aggregate']}\")\n",
    "        return {\"aggregate\": [self._value]}\n",
    "\n",
    "\n",
    "# 상태 그래프 초기화\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\n",
    "builder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\n",
    "builder.add_node(\"d\", ReturnNodeValue(\"I'm D\"))\n",
    "builder.add_node(\"e\", ReturnNodeValue(\"I'm E\"))\n",
    "\n",
    "\n",
    "# 상태의 'which' 값에 따른 조건부 라우팅 경로 결정 함수\n",
    "def route_bc_or_cd(state: State) -> Sequence[str]:\n",
    "    if state[\"which\"] == \"cd\":\n",
    "        return [\"c\", \"d\"]\n",
    "    elif state[\"which\"] == \"bc\":\n",
    "        return [\"b\", \"c\"]\n",
    "    else:\n",
    "        return [\"b\", \"c\", \"d\"]\n",
    "\n",
    "\n",
    "# 전체 병렬 처리할 노드 목록\n",
    "intermediates = [\"b\", \"c\", \"d\"]\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"a\",\n",
    "    route_bc_or_cd,\n",
    "    intermediates,\n",
    ")\n",
    "for node in intermediates:\n",
    "    builder.add_edge(node, \"e\")\n",
    "\n",
    "\n",
    "# 최종 노드 연결 및 그래프 컴파일\n",
    "builder.add_edge(\"e\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행(which: bc 로 지정)\n",
    "result = graph.invoke({\"aggregate\": [], \"which\": \"bc\"})\n",
    "print(\"===\" * 30)\n",
    "print(result[\"aggregate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행(which: cd 로 지정)\n",
    "result = graph.invoke({\"aggregate\": [], \"which\": \"cd\"})\n",
    "print(\"===\" * 30)\n",
    "print(result[\"aggregate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6.대화 기록 요약을 추가하는 방법\n",
    "\n",
    "대화 기록을 유지하는 것은 **지속성**의 가장 일반적인 사용 사례 중 하나입니다. 이는 대화를 지속하기 쉽게 만들어주는 장점이 있습니다. \n",
    "\n",
    "하지만 대화가 길어질수록 대화 기록이 누적되어 `context window`를 더 많이 차지하게 됩니다. 이는 `LLM` 호출이 더 비싸고 길어지며, 잠재적으로 오류가 발생할 수 있어 바람직하지 않을 수 있습니다. 이를 해결하기 위한 한 가지 방법은 현재까지의 대화 요약본을 생성하고, 이를 최근 `N` 개의 메시지와 함께 사용하는 것입니다. \n",
    "\n",
    "이 가이드에서는 이를 구현하는 방법의 예시를 살펴보겠습니다.\n",
    "\n",
    "다음과 같은 단계가 필요합니다.\n",
    "\n",
    "- 대화가 너무 긴지 확인 (메시지 수나 메시지 길이로 확인 가능)\n",
    "- 너무 길다면 요약본 생성 (이를 위한 프롬프트 필요)\n",
    "- 마지막 `N` 개의 메시지를 제외한 나머지 삭제\n",
    "\n",
    "이 과정에서 중요한 부분은 오래된 메시지를 삭제(`DeleteMessage`) 하는 것입니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, RemoveMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# 대화 및 요약을 위한 모델 초기화\n",
    "model = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "# 메시지 상태와 요약 정보를 포함하는 상태 클래스\n",
    "class State(MessagesState):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM 답변 생성 노드를 구현합니다. 여기서 이전의 대화요약 내용이 있다면 이를 입력에 포함합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: State):\n",
    "    # 이전 요약 정보 확인\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # 이전 요약 정보가 있다면 시스템 메시지로 추가\n",
    "    if summary:\n",
    "        # 시스템 메시지와 이전 메시지 결합\n",
    "        messages = [\n",
    "            SystemMessage(content=f\"Summary of conversation earlier: {summary}\")\n",
    "        ] + state[\"messages\"]\n",
    "    else:\n",
    "        # 이전 메시지만 사용\n",
    "        messages = state[\"messages\"]\n",
    "\n",
    "    # 모델 호출\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # 응답 반환\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요약이 필요한 상황인지를 판단합니다.\n",
    "\n",
    "여기서는 메시지 수가 6개 초과라면 요약 노드로 이동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "# 대화 종료 또는 요약 결정 로직\n",
    "def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
    "    # 메시지 목록 확인\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # 메시지 수가 6개 초과라면 요약 노드로 이동\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요약 노드를 구현합니다. 이전 요약 정보가 있다면 이를 입력에 포함하고, 없다면 새로운 요약 메시지를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화 내용 요약 및 메시지 정리 로직\n",
    "def summarize_conversation(state: State):\n",
    "    # 이전 요약 정보 확인\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # 이전 요약 정보가 있다면 요약 메시지 생성\n",
    "    if summary:\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above in Korean.\"\n",
    "        )\n",
    "    else:\n",
    "        # 요약 메시지 생성\n",
    "        summary_message = \"Create a summary of the conversation above in Korean:\"\n",
    "\n",
    "    # 요약 메시지와 이전 메시지 결합\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    # 모델 호출\n",
    "    response = model.invoke(messages)\n",
    "    # 오래된 메시지 삭제\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    # 요약 정보 반환\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프 생성 및 컴파일\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워크플로우 그래프 초기화\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# 대화 및 요약 노드 추가\n",
    "workflow.add_node(\"conversation\", generate)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# 시작점을 대화 노드로 설정\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "\n",
    "# 조건부 엣지 추가\n",
    "workflow.add_conditional_edges(\n",
    "    \"conversation\",\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# 요약 노드에서 종료 노드로의 엣지 추가\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# 워크플로우 컴파일 및 메모리 체크포인터 설정\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용자 메시지 출력을 위한 함수를 구현(헬퍼 함수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_user_message(message):\n",
    "    print(\"\\n==================================================\\n\\n😎\", message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대화를 시작합니다. 우선 6개의 대화를 채워보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메시지 핸들링을 위한 HumanMessage 클래스 임포트\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 스레드 ID가 포함된 설정 객체 초기화\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"resursion_limit\": 10}}\n",
    "\n",
    "# 첫 번째 메시지\n",
    "print_user_message(\"안녕하세요? 반갑습니다. 제 이름은 테디입니다.\")\n",
    "stream_graph(\n",
    "    app,\n",
    "    {\"messages\": [(\"human\", \"안녕하세요? 반갑습니다. 제 이름은 테디입니다.\")]},\n",
    "    config,\n",
    ")\n",
    "\n",
    "# 두 번째 메시지\n",
    "print_user_message(\"제 이름이 뭔지 기억하세요?\")\n",
    "stream_graph(app, {\"messages\": [(\"human\", \"제 이름이 뭔지 기억하세요?\")]}, config)\n",
    "\n",
    "# 세 번째 메시지\n",
    "print_user_message(\"제 취미는 Netflix 시리즈를 보는 것입니다.\")\n",
    "stream_graph(\n",
    "    app, {\"messages\": [(\"human\", \"제 취미는 Netflix 시리즈를 보는 것입니다.\")]}, config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 확인합니다. 6개 대화를 했으므로, 요약본이 만들어 져야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 구성 값 검색\n",
    "values = app.get_state(config).values\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 추가로 대화를 입력하여 요약본을 기반으로 잘 답변하는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네 번째 메시지\n",
    "print_user_message(\"제 취미가 뭐라고 했나요?\")\n",
    "stream_graph(app, {\"messages\": [(\"human\", \"제 취미가 뭐라고 했나요?\")]}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추적: https://smith.langchain.com/public/c0f62f0b-74b5-4dc3-bd1c-3e474a0dbef9/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7. Human in the Loop\n",
    "\n",
    "LLM 애플리케이션에서 Human-in-the-loop 은 자동화된 AI 시스템과 인간의 개입 및 판단을 결합하는 접근 방식입니다. \n",
    "\n",
    "이 방식에서는 AI 시스템이 초기 처리와 분석을 수행하지만, 불확실하거나 중요한 결정이 필요한 시점에서 인간 전문가의 개입을 요청합니다. \n",
    "\n",
    "Human-in-the-loop 은 높은 정확도가 필요한 복잡한 상황, 윤리적 판단이 필요한 경우, 또는 AI의 신뢰도가 낮은 결과에 대해 검증이 필요할 때 특히 중요합니다.\n",
    "\n",
    "`from langgraph.types import interrupt`\n",
    "\n",
    "`interrupt` 함수는 인간의 개입을 요청하는 데 사용됩니다. 이 함수는 인간의 입력을 받아 처리하고, 결과를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "import uuid\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    evaluation: Annotated[str, \"Evaluation\"]\n",
    "\n",
    "\n",
    "def llm_node(state: State):\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def human_node(state: State):\n",
    "    value = interrupt(\n",
    "        # 사람의 피드백 요청\n",
    "        {\"text_to_revise\": state[\"messages\"][-1]}\n",
    "    )\n",
    "    return {\n",
    "        # 사람의 피드백으로 업데이트\n",
    "        \"messages\": [HumanMessage(content=value)]\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluation_node(state: State):\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "    prompt = f\"\"\"\n",
    "Here is the user's question and the model's response\n",
    "The user's question: {state[\"messages\"][0]}\n",
    "Model's response: {state[\"messages\"][-1]}\n",
    "\n",
    "Please rate whether the model's response accurately answered your question.\"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"evaluation\": response}\n",
    "\n",
    "\n",
    "# 그래프 생성\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# 노드 추가\n",
    "workflow.add_node(\"llm\", llm_node)\n",
    "workflow.add_node(\"human\", human_node)\n",
    "workflow.add_node(\"eval\", evaluation_node)\n",
    "\n",
    "# 엣지 추가\n",
    "workflow.add_edge(START, \"llm\")\n",
    "workflow.add_edge(\"llm\", \"human\")\n",
    "workflow.add_edge(\"human\", \"eval\")\n",
    "workflow.add_edge(\"eval\", END)\n",
    "\n",
    "# 체크포인터 설정\n",
    "checkpointer = MemorySaver()\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기 질문을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 설정\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "\n",
    "stream_graph(app, {\"messages\": [(\"human\", \"2+6=?\")]}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Command` 객체는 인간의 개입을 요청하는 데 사용됩니다. 이 객체는 인간의 입력을 받아 처리하고, 결과를 반환합니다.\n",
    "\n",
    "- `resume`: 피드백을 입력하고 남은 단계를 재게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(app, Command(resume=\"2 + 6 = 9\"), config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (인터럽트) 추적: https://smith.langchain.com/public/c251955b-6999-4983-9a92-6905700333d3/r\n",
    "- (이후) 추적: https://smith.langchain.com/public/c251955b-6999-4983-9a92-6905700333d3/r    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "import uuid\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.constants import START\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    evaluation: Annotated[str, \"Evaluation\"]\n",
    "\n",
    "\n",
    "def llm_node(state: State):\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def human_approval(state: State) -> Command[Literal[\"llm\", END]]:\n",
    "    is_approved = interrupt(\n",
    "        {\n",
    "            \"question\": \"Is this correct?\",\n",
    "            # 답변을 사람에게 보여주고 수정 요청\n",
    "            \"need_to_revise\": state[\"messages\"][-1],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if is_approved:\n",
    "        return Command(\n",
    "            goto=END, update={\"messages\": [HumanMessage(content=\"You are great!\")]}\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=\"llm\",\n",
    "            update={\n",
    "                \"messages\": [HumanMessage(content=\"You are wrong.. Please try again\")]\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "# 그래프 생성\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# 노드 추가\n",
    "workflow.add_node(\"llm\", llm_node)\n",
    "workflow.add_node(\"human\", human_approval)\n",
    "\n",
    "# 엣지 추가\n",
    "workflow.add_edge(START, \"llm\")\n",
    "workflow.add_edge(\"llm\", \"human\")\n",
    "\n",
    "# 체크포인터 설정\n",
    "checkpointer = MemorySaver()\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 설정\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "\n",
    "stream_graph(app, {\"messages\": [(\"human\", \"1, 2, 4, 10, ?\")]}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_graph(app, Command(resume=False), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8. Long-Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**메모리**는 사람들이 현재와 미래를 이해하기 위해 정보를 저장하고, 검색하며 사용하는 인지 기능입니다.\n",
    "\n",
    "AI 애플리케이션에서 사용할 수 있는 [다양한 **장기 메모리 유형**](https://langchain-ai.github.io/langgraph/concepts/memory/#memory)이 있습니다.\n",
    "\n",
    "여기에서는 **장기 기억**을 저장하고 검색하는 방법으로 [LangGraph Memory Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore)를 소개합니다.\n",
    "\n",
    "`short-term (within-thread)` 및 `long-term (across-thread)` 메모리를 모두 사용하는 챗봇을 구축할 것입니다.\n",
    "\n",
    "사용자에 대한 사실인 장기 [**semantic memory**](https://langchain-ai.github.io/langgraph/concepts/memory/#semantic-memory)에 중점을 둘 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangGraph Store**\n",
    "\n",
    "[LangGraph Memory Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) 는 LangGraph에서 *thread 간* 정보를 저장하고 검색할 수 있는 방법을 제공합니다. \n",
    "\n",
    "이 저장소는 지속적인 `key-value` 데이터를 관리하기 위한 [오픈 소스 기본 클래스](https://blog.langchain.dev/launching-long-term-memory-support-in-langgraph/)로, 개발자가 자신만의 저장소를 쉽게 구현할 수 있도록 설계되었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# LangGraph의 InMemoryStore 인스턴스 생성\n",
    "persistant_memory = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스토어에 객체(예: 메모리)를 저장할 때는 [Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore)에 다음 정보를 제공합니다.\n",
    "\n",
    "- **namespace** (`namespace`): 객체를 구분하는 데 사용되는 튜플 형태의 식별자입니다 (디렉토리와 유사).\n",
    "- **key** (`key`): 객체의 고유 식별자입니다 (파일 이름과 유사).\n",
    "- **value** (`value`): 객체의 실제 내용입니다 (파일 내용과 유사).\n",
    "\n",
    "`namespace`와 `key`를 사용하여 객체를 스토어에 저장하기 위해 [put](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.put) 메서드를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장할 메모리의 사용자 ID 설정\n",
    "user_id = \"teddy\"\n",
    "\n",
    "# 사용자 ID와 메모리 구분을 위한 네임스페이스 정의\n",
    "namespace_for_memory = (\"memories\", user_id)\n",
    "\n",
    "# 고유 키 생성을 위한 UUID 생성\n",
    "key = \"user_memory\"\n",
    "\n",
    "# 저장할 메모리 값으로 딕셔너리 정의\n",
    "value = {\n",
    "    \"job\": \"AI Engineer\",\n",
    "    \"location\": \"Seoul, Korea\",\n",
    "    \"hobbies\": [\"Watching Netflix\", \"Coding\"],\n",
    "}\n",
    "\n",
    "# 지정된 네임스페이스에 메모리 저장\n",
    "persistant_memory.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`search`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.search)을 이용하여 `namespace` 기준으로 `store`에서 객체를 검색할 수 있습니다. 이 메서드는 목록을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지정된 네임스페이스로부터 메모리 객체 검색\n",
    "memories = persistant_memory.search(namespace_for_memory)\n",
    "\n",
    "# 검색된 메모리 객체의 메타데이터를 딕셔너리 형식으로 변환\n",
    "memories[0].dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**장기 메모리를 갖춘 챗봇**\n",
    "\n",
    "챗봇은 [두 가지 유형의 메모리](https://docs.google.com/presentation/d/181mvjlgsnxudQI6S3ritg9sooNyu4AcLLFH1UK0kIuk/edit#slide=id.g30eb3c8cf10_0_156)를 갖추어야 합니다.\n",
    "\n",
    "1. `Short-term (within-thread) memory`: 챗봇이 대화 내역을 지속적으로 저장하거나, 대화 세션 중에 중단을 허용할 수 있습니다.\n",
    "2. `Long-term (cross-thread) memory`: 챗봇이 특정 사용자에 대한 정보를 *모든 대화 세션에 걸쳐* 기억할 수 있습니다.\n",
    "\n",
    "이 두 가지 메모리 유형을 통해 챗봇은 사용자와의 대화를 더욱 원활하고 개인화된 방식으로 관리할 수 있습니다. `Short-term memory`는 현재 대화 세션 내에서의 맥락을 유지하는 데 사용되며, `Long-term memory`는 사용자에 대한 지속적인 정보를 저장하여 여러 세션에 걸쳐 일관된 상호작용을 가능하게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 모델 초기화\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 모델 시스템 메시지 정의\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user. \n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "Here is the memory (it may be empty): {memory}\"\"\"\n",
    "\n",
    "# 새로운 메모리 생성 지침 정의\n",
    "CREATE_MEMORY_INSTRUCTION = \"\"\"\"You are collecting information about the user to personalize your responses.\n",
    "\n",
    "CURRENT USER INFORMATION:\n",
    "{memory}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Review the chat history below carefully\n",
    "2. Identify new information about the user, such as:\n",
    "   - Personal details (name, job, location)\n",
    "   - Preferences (likes, dislikes)\n",
    "   - Interests and hobbies\n",
    "   - Past experiences\n",
    "   - Goals or future plans\n",
    "3. Merge any new information with existing memory\n",
    "4. Format the memory as a clear, bulleted list\n",
    "5. If new information conflicts with existing memory, keep the most recent version\n",
    "\n",
    "Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.\n",
    "\n",
    "Based on the chat history below, please update the user information:\"\"\"\n",
    "\n",
    "\n",
    "# call_model 함수 정의\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Load memory from the store and use it to personalize the chatbot's response.\"\"\"\n",
    "\n",
    "    # 설정에서 사용자 ID 가져오기\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # 스토어에서 메모리 검색\n",
    "    namespace = (\"memories\", user_id)\n",
    "    key = \"user_memory\"\n",
    "    existing_memory = store.get(namespace, key)\n",
    "\n",
    "    # 기존 메모리 내용 추출 및 프리픽스 추가\n",
    "    if existing_memory:\n",
    "        # 값은 메모리 키를 포함하는 딕셔너리\n",
    "        existing_memory_content = existing_memory.value.get(\"memories\")\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\"\n",
    "\n",
    "    # 시스템 프롬프트에 메모리 포맷\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=existing_memory_content)\n",
    "\n",
    "    # 메모리와 대화 기록을 사용하여 응답 생성\n",
    "    response = model.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# write_memory 함수 정의\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Reflect on the chat history and save a memory to the store.\"\"\"\n",
    "\n",
    "    # 설정에서 사용자 ID 가져오기\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # 스토어에서 기존 메모리 검색\n",
    "    namespace = (\"memories\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # 메모리 추출\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get(\"memories\")\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\"\n",
    "\n",
    "    # 시스템 프롬프트에 메모리 포맷\n",
    "    system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=existing_memory_content)\n",
    "    new_memory = model.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n",
    "\n",
    "    # 스토어에 기존 메모리 덮어쓰기\n",
    "    key = \"user_memory\"\n",
    "\n",
    "    # 메모리 키를 포함하는 딕셔너리로 값 작성\n",
    "    store.put(namespace, key, {\"memories\": new_memory.content})\n",
    "\n",
    "\n",
    "# 그래프 정의\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"call_model\", call_model)\n",
    "workflow.add_node(\"write_memory\", write_memory)\n",
    "workflow.add_edge(START, \"call_model\")\n",
    "workflow.add_edge(\"call_model\", \"write_memory\")\n",
    "workflow.add_edge(\"write_memory\", END)\n",
    "\n",
    "# 장기 메모리 저장소 설정\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# 단기 메모리 체크포인터 설정\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# 체크포인터 및 스토어를 포함하여 그래프 컴파일\n",
    "app = workflow.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# 그래프 시각화\n",
    "visualize_graph(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단기 메모리를 위한 쓰레드 ID 제공(thread_id)\n",
    "# 장기 메모리를 위한 사용자 ID 제공(user_id)\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"teddy\"}}\n",
    "\n",
    "# 사용자 입력\n",
    "input_messages = [\n",
    "    HumanMessage(\n",
    "        content=\"안녕 반가워! 내 이름은 테디 입니다. 저의 취미는 코딩 하고 영화 보는 것입니다.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# 그래프 실행\n",
    "stream_graph(app, {\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단기 메모리를 위한 쓰레드 ID 제공(thread_id)\n",
    "# 장기 메모리를 위한 사용자 ID 제공(user_id)\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"teddy2\"}}\n",
    "\n",
    "# 사용자 입력\n",
    "input_messages = [HumanMessage(content=\"내 취미가 뭐였더라...\")]\n",
    "\n",
    "# 그래프 실행\n",
    "stream_graph(app, {\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단기 메모리를 위한 쓰레드 ID 제공(thread_id)\n",
    "# 장기 메모리를 위한 사용자 ID 제공(user_id)\n",
    "config = {\"configurable\": {\"thread_id\": \"3\", \"user_id\": \"teddy\"}}\n",
    "\n",
    "# 사용자 입력\n",
    "input_messages = [HumanMessage(content=\"내 취미가 뭐였더라...\")]\n",
    "\n",
    "# 그래프 실행\n",
    "stream_graph(app, {\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistant_memory.get((\"memories\", \"teddy\"), \"user_memory\").value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단기 메모리를 위한 쓰레드 ID 제공(thread_id)\n",
    "# 장기 메모리를 위한 사용자 ID 제공(user_id)\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"john\"}}\n",
    "\n",
    "# 사용자 입력\n",
    "input_messages = [HumanMessage(content=\"안녕 반가워! 혹시 내 취미 기억해?\")]\n",
    "\n",
    "# 그래프 실행\n",
    "stream_graph(app, {\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단기 메모리를 위한 쓰레드 ID 제공(thread_id)\n",
    "# 장기 메모리를 위한 사용자 ID 제공(user_id)\n",
    "config = {\"configurable\": {\"thread_id\": \"3\", \"user_id\": \"teddy\"}}\n",
    "\n",
    "# 사용자 입력\n",
    "input_messages = [HumanMessage(content=\"나에 대해 아는 정보 모두 말해줘\")]\n",
    "\n",
    "# 그래프 실행\n",
    "stream_graph(app, {\"messages\": input_messages}, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추적: https://smith.langchain.com/public/618341ed-4bc2-443f-8ad0-f96fc464fac8/r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
